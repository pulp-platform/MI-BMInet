{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC feature extraction and Network training\n",
    "\n",
    "In this notebook you will go through an example flow of processing audio data, complete with feature extraction and training.\n",
    "\n",
    "Make sure you read the instructions on the exercise sheet and follow the task order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert(tf.__version__ == \"2.1.0\")\n",
    "assert(tf.executing_eagerly() == True)\n",
    "\n",
    "DataSetPath = \"../hey_snips_kws_4.0/hey_snips_research_6k_en_train_eval_clean_ter/\"\n",
    "\n",
    "with open(DataSetPath+\"train.json\") as jsonfile:\n",
    "    traindata = json.load(jsonfile)\n",
    "\n",
    "with open(DataSetPath+\"test.json\") as jsonfile:\n",
    "    testdata = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    totalSliceLength = 10 # Length to stuff the signals to, given in seconds\n",
    "\n",
    "    # trainsize = len(traindata) # Number of loaded training samples\n",
    "    # testsize = len(testdata) # Number of loaded testing samples\n",
    "\n",
    "    trainsize = 1000 # Number of loaded training samples\n",
    "    testsize = 100 # Number of loaded testing samples\n",
    "\n",
    "\n",
    "    fs = 16000 # Sampling rate of the samples\n",
    "    segmentLength = 1024 # Number of samples to use per segment\n",
    "\n",
    "    sliceLength = int(totalSliceLength * fs / segmentLength)*segmentLength\n",
    "\n",
    "    for i in tqdm(range(trainsize)): \n",
    "        fs, train_sound_data = wavfile.read(DataSetPath+traindata[i]['audio_file_path']) # Read wavfile to extract amplitudes\n",
    "\n",
    "        _x_train = train_sound_data.copy() # Get a mutable copy of the wavfile\n",
    "        _x_train.resize(sliceLength) # Zero stuff the single to a length of sliceLength\n",
    "        _x_train = _x_train.reshape(-1,int(segmentLength)) # Split slice into Segments with 0 overlap\n",
    "        x_train_list.append(_x_train.astype(np.float32)) # Add segmented slice to training sample list, cast to float so librosa doesn't complain\n",
    "        y_train_list.append(traindata[i]['is_hotword']) # Read label \n",
    "\n",
    "    for i in tqdm(range(testsize)):\n",
    "        fs, test_sound_data = wavfile.read(DataSetPath+testdata[i]['audio_file_path'])\n",
    "        _x_test = test_sound_data.copy()\n",
    "        _x_test.resize(sliceLength)\n",
    "        _x_test = _x_test.reshape((-1,int(segmentLength)))\n",
    "        x_test_list.append(_x_test.astype(np.float32))\n",
    "        y_test_list.append(testdata[i]['is_hotword'])\n",
    "\n",
    "    x_train = tf.convert_to_tensor(np.asarray(x_train_list))\n",
    "    y_train = tf.convert_to_tensor(np.asarray(y_train_list))\n",
    "\n",
    "    x_test = tf.convert_to_tensor(np.asarray(x_test_list))\n",
    "    y_test = tf.convert_to_tensor(np.asarray(y_test_list))\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfccs(tensor):\n",
    "    sample_rate = 16000.0\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    frame_length = 1024\n",
    "    num_mfcc = 13\n",
    "\n",
    "    stfts = tf.signal.stft(tensor, frame_length=frame_length, frame_step=frame_length, fft_length=frame_length)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "    spectrograms = tf.reshape(spectrograms, (spectrograms.shape[0],spectrograms.shape[1],-1))\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "      num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
    "      upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfcc]\n",
    "    return tf.reshape(mfccs, (mfccs.shape[0],mfccs.shape[1],mfccs.shape[2],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mfcc = compute_mfccs(x_train)\n",
    "x_test_mfcc = compute_mfccs(x_test)\n",
    "\n",
    "print(x_train_mfcc.shape)\n",
    "print(x_test_mfcc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 10\n",
    "epochs = 30\n",
    "\n",
    "train_set = (x_train_mfcc/512 + 0.5)\n",
    "train_labels = y_train\n",
    "\n",
    "test_set = (x_test_mfcc/512 + 0.5)\n",
    "test_labels = y_test\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(train_set, y_train, batchSize, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.evaluate(test_set, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MFCCmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NNoM Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tian/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /Users/tian/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "input_1 max value: 1.0 min value: 0.0 dec bit 7\n",
      "KLD loss [8.512857792631207, 0.05423308028116321, 3.5350372574248023, 10.68689804271818]\n",
      "KLD shift [2, 3, 4, 5]\n",
      "conv2d_1 is using KLD method, original shift 2 KLD results 3\n",
      "conv2d_1 max value: 18.657698 min value: -17.214493 dec bit 3\n",
      "KLD loss [1.8572619425097299, 0.03126832680802451, 0.2883232057864603, 1.8431031639186313]\n",
      "KLD shift [3, 4, 5, 6]\n",
      "batch_normalization_1 is using KLD method, original shift 3 KLD results 4\n",
      "batch_normalization_1 max value: 9.7006035 min value: -14.863518 dec bit 4\n",
      "KLD loss [3.346329867044317, 0.2793355000830491, 1.250332911578891, 2.6988595963244317]\n",
      "KLD shift [3, 4, 5, 6]\n",
      "depthwise_conv2d_1 is using KLD method, original shift 3 KLD results 4\n",
      "depthwise_conv2d_1 max value: 15.94808 min value: -14.485238 dec bit 4\n",
      "KLD loss [8.719743910656735, 0.04939034764719919, 0.04512962745179024, 0.11413294517521932]\n",
      "KLD shift [2, 3, 4, 5]\n",
      "batch_normalization_2 is using KLD method, original shift 2 KLD results 4\n",
      "batch_normalization_2 max value: 17.641071 min value: -16.831543 dec bit 4\n",
      "KLD loss [8.719743910656735, 0.04939034764719919, 0.04512962745179024, 0.11413294517521932]\n",
      "KLD shift [2, 3, 4, 5]\n",
      "activation_1 is using KLD method, original shift 2 KLD results 4\n",
      "activation_1 max value: 17.641071 min value: -16.831543 dec bit 4\n",
      "KLD loss [8.719743910656735, 0.04939034764719919, 0.04512962745179024, 0.11413294517521932]\n",
      "KLD shift [2, 3, 4, 5]\n",
      "average_pooling2d_1 is using KLD method, original shift 2 KLD results 4\n",
      "average_pooling2d_1 max value: 17.641071 min value: -16.831543 dec bit 4\n",
      "KLD loss [8.719743910656735, 0.04939034764719919, 0.04512962745179024, 0.11413294517521932]\n",
      "KLD shift [2, 3, 4, 5]\n",
      "dropout_1 is using KLD method, original shift 2 KLD results 4\n",
      "dropout_1 max value: 17.641071 min value: -16.831543 dec bit 4\n",
      "KLD loss [3.2837956945427114, 0.05686116380445935, 0.05261631458972486, 0.5440320245251424]\n",
      "KLD shift [0, 1, 2, 3]\n",
      "separable_conv2d_1 is using KLD method, original shift 0 KLD results 2\n",
      "separable_conv2d_1 max value: 61.01077 min value: -102.81992 dec bit 2\n",
      "KLD loss [1.9855256028123516, 0.03870266132122911, 0.03994543472792475, 0.44816715283219544]\n",
      "KLD shift [1, 2, 3, 4]\n",
      "batch_normalization_3 is using KLD method, original shift 1 KLD results 2\n",
      "batch_normalization_3 max value: 29.759659 min value: -57.321606 dec bit 2\n",
      "KLD loss [1.9855256028123516, 0.03870266132122911, 0.03994543472792475, 0.44816715283219544]\n",
      "KLD shift [1, 2, 3, 4]\n",
      "activation_2 is using KLD method, original shift 1 KLD results 2\n",
      "activation_2 max value: 29.759659 min value: -57.321606 dec bit 2\n",
      "KLD loss [1.9855256028123516, 0.03870266132122911, 0.03994543472792475, 0.44816715283219544]\n",
      "KLD shift [1, 2, 3, 4]\n",
      "average_pooling2d_2 is using KLD method, original shift 1 KLD results 2\n",
      "average_pooling2d_2 max value: 29.759659 min value: -57.321606 dec bit 2\n",
      "KLD loss [1.9855256028123516, 0.03870266132122911, 0.03994543472792475, 0.44816715283219544]\n",
      "KLD shift [1, 2, 3, 4]\n",
      "dropout_2 is using KLD method, original shift 1 KLD results 2\n",
      "dropout_2 max value: 29.759659 min value: -57.321606 dec bit 2\n",
      "KLD loss [1.9855256028123516, 0.03870266132122911, 0.03994543472792475, 0.44816715283219544]\n",
      "KLD shift [1, 2, 3, 4]\n",
      "flatten is using KLD method, original shift 1 KLD results 2\n",
      "flatten max value: 29.759659 min value: -57.321606 dec bit 2\n",
      "dense max value: 4.5554075 min value: -4.5554047 dec bit 4\n",
      "softmax max value: 0.9998895 min value: 0.000110452755 dec bit 7\n",
      "shift list {'input_1': 7, 'conv2d_1': 4, 'batch_normalization_1': 4, 'depthwise_conv2d_1': 4, 'batch_normalization_2': 4, 'activation_1': 4, 'average_pooling2d_1': 4, 'dropout_1': 4, 'separable_conv2d_1': 2, 'batch_normalization_3': 2, 'activation_2': 2, 'average_pooling2d_2': 2, 'dropout_2': 2, 'flatten': 2, 'dense': 4, 'softmax': 7}\n",
      "weights for layer conv2d_1\n",
      "  weight: conv2d_1/kernel:0\n",
      "  original shape:  (1, 43, 1, 8)\n",
      "  dec bit 6\n",
      "  reshape to: (8, 1, 43, 1)\n",
      "weights for layer batch_normalization_1\n",
      "weights for layer depthwise_conv2d_1\n",
      "  weight: depthwise_conv2d_1/depthwise_kernel:0\n",
      "  original shape:  (38, 1, 8, 2)\n",
      "  dec bit 7\n",
      "  reshape to: (2, 38, 1, 8)\n",
      "weights for layer batch_normalization_2\n",
      "weights for layer separable_conv2d_1\n",
      "  weight: separable_conv2d_1/depthwise_kernel:0\n",
      "  original shape:  (1, 16, 16, 1)\n",
      "  dec bit 6\n",
      "  reshape to: (1, 1, 16, 16)\n",
      "  weight: separable_conv2d_1/pointwise_kernel:0\n",
      "  original shape:  (1, 16, 16, 1)\n",
      "  dec bit 6\n",
      "  reshape to: (1, 1, 16, 16)\n",
      "weights for layer batch_normalization_3\n",
      "weights for layer dense\n",
      "  weight: dense/kernel:0\n",
      "  original shape:  (32, 2)\n",
      "  dec bit 10\n",
      "  reshape to: (2, 32)\n",
      "  bias:  dense/bias:0\n",
      "  original shape:  (2,)\n",
      "  dec bit 9\n",
      "  reshape to: (2,)\n",
      "1. ###### layer <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x147453410>\n",
      "2. ###### layer <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x147453410>\n",
      "1. ###### layer <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1474537d0>\n",
      "2. ###### layer <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1474537d0>\n",
      "1. ###### layer <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x147453790>\n",
      "1. ###### layer <tensorflow.python.keras.layers.convolutional.DepthwiseConv2D object at 0x148b896d0>\n",
      "2. ###### layer <tensorflow.python.keras.layers.convolutional.DepthwiseConv2D object at 0x148b896d0>\n",
      "1. ###### layer <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x148b89d50>\n",
      "1. ###### layer <tensorflow.python.keras.layers.core.Activation object at 0x147453990>\n",
      "1. ###### layer <tensorflow.python.keras.layers.pooling.AveragePooling2D object at 0x1474533d0>\n",
      "1. ###### layer <tensorflow.python.keras.layers.core.Dropout object at 0x148ba3550>\n",
      "1. ###### layer <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x148ba3610>\n",
      "2. ###### layer <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x148ba3610>\n",
      "1. ###### layer <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x148ba3c50>\n",
      "1. ###### layer <tensorflow.python.keras.layers.core.Activation object at 0x148b89fd0>\n",
      "1. ###### layer <tensorflow.python.keras.layers.pooling.AveragePooling2D object at 0x148ba5310>\n",
      "1. ###### layer <tensorflow.python.keras.layers.core.Dropout object at 0x148ba5690>\n",
      "1. ###### layer <tensorflow.python.keras.layers.core.Flatten object at 0x148ba5750>\n",
      "1. ###### layer <tensorflow.python.keras.layers.core.Dense object at 0x148ba58d0>\n",
      "2. ###### layer <tensorflow.python.keras.layers.core.Dense object at 0x148ba58d0>\n",
      "3. ###### layer <tensorflow.python.keras.layers.core.Dense object at 0x148ba58d0>\n",
      "1. ###### layer <tensorflow.python.keras.layers.core.Activation object at 0x148ba5dd0>\n",
      "2. ###### layer <tensorflow.python.keras.layers.core.Activation object at 0x148ba5dd0>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nnom_utils import * \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "npzfile = np.load(\"../../data_nnom/2class_mm_intra_test_split1.npz\")\n",
    "x_test, y_test = npzfile['X_Train'], npzfile['y_Train']\n",
    "    \n",
    "model = keras.models.load_model(\"../../model_nnom/global_class_2_ds3_nch38_T1_split_1_v1.h5\")\n",
    "generate_model(model, x_test, format='hwc', name=\"weights.h\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary test file generated: test_data.bin\n",
      "test data length: 1470\n",
      "1470/1470 - 0s - loss: 0.3678 - acc: 0.8422\n",
      "Test loss: 0.3677783435299283\n",
      "Top 1: 0.84217685\n",
      "Top 2: [1. 1. 1. ... 1. 1. 1.]\n",
      "[[624 111]\n",
      " [121 614]]\n",
      "conv2d_1_1/kernel:0 Dec num: 6\n",
      "batch_normalization_1_1/gamma:0 Dec num: 12\n",
      "batch_normalization_1_1/beta:0 Dec num: 12\n",
      "batch_normalization_1_1/moving_mean:0 Dec num: 12\n",
      "batch_normalization_1_1/moving_variance:0 Dec num: 12\n",
      "depthwise_conv2d_1_1/depthwise_kernel:0 Dec num: 7\n",
      "batch_normalization_2_1/gamma:0 Dec num: 7\n",
      "batch_normalization_2_1/beta:0 Dec num: 7\n",
      "batch_normalization_2_1/moving_mean:0 Dec num: 7\n",
      "batch_normalization_2_1/moving_variance:0 Dec num: 7\n",
      "separable_conv2d_1_1/depthwise_kernel:0 Dec num: 6\n",
      "separable_conv2d_1_1/pointwise_kernel:0 Dec num: 6\n",
      "batch_normalization_3_1/gamma:0 Dec num: 8\n",
      "batch_normalization_3_1/beta:0 Dec num: 8\n",
      "batch_normalization_3_1/moving_mean:0 Dec num: 8\n",
      "batch_normalization_3_1/moving_variance:0 Dec num: 8\n",
      "dense_1/kernel:0 Dec num: 10\n",
      "dense_1/bias:0 Dec num: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nnom_utils import * \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "npzfile = np.load(\"../../data_nnom/2class_mm_intra_test_split1.npz\")\n",
    "x_test, y_test = npzfile['X_Train'], npzfile['y_Train']\n",
    "y_test = to_categorical(y_test)\n",
    "    \n",
    "model = keras.models.load_model(\"../../model_nnom/global_class_2_ds3_nch38_T1_split_1_v1.h5\")\n",
    "generate_test_bin(x_test*127, y_test, name='test_data.bin')\n",
    "scores = evaluate_model(model,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3677783435299283, 0.84217685]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
